{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "import datetime\n",
    "\n",
    "from loglizer.loglizer.models.DeepLog import DeepLog\n",
    "from loglizer.loglizer.models.PCA import PCA\n",
    "from loglizer.loglizer.models.LOF import LocalOutlierFactor as LOF\n",
    "from loglizer.loglizer.models.KNeighbors import KNeighbors as KNN\n",
    "from loglizer.loglizer.models.LogClustering import LogClustering\n",
    "from loglizer.loglizer.models.InvariantsMiner import InvariantsMiner\n",
    "from loglizer.loglizer.models.IsolationForest import IsolationForest\n",
    "from loglizer.loglizer.models import DeepLog\n",
    "from loglizer.loglizer.preprocessing import Vectorizer, Iterator\n",
    "from loglizer.loglizer import preprocessing\n",
    "from loglizer.loglizer.dataloader import HDFS, BGL, Thunderbird\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDatasets():\n",
    "    datasets = {}\n",
    "    \n",
    "    #Thunderbird\n",
    "    print(\"load Thunderbird\")\n",
    "    (x_train, y_train), (x_test, y_test) = Thunderbird.loadDataset('Drain_result/Thunderbird_10m.log_structured.csv', window='sliding', time_interval=3600*0.5, stepping_size=60*5, train_ratio=0.7)\n",
    "    datasets['Thunderbird'] = {'x_train':x_train, 'y_train': y_train, 'x_test': x_test, 'y_test':y_test}\n",
    "    \n",
    "\n",
    "    \n",
    "    #HDFS\n",
    "    print(\"load HDFS\")\n",
    "    struct_log = 'Drain_result/HDFS.log_structured.csv'\n",
    "    labels = 'logs/HDFS/anomaly_label.csv'\n",
    "    (x_train, y_train), (x_test, y_test) = HDFS.loadDataset(struct_log,\n",
    "                                                           label_file=labels,\n",
    "                                                           window='session',\n",
    "                                                           train_ratio=0.7,\n",
    "                                                           split_type='uniform')\n",
    "    \n",
    "    datasets['HDFS'] = {'x_train':x_train, \n",
    "                        'y_train': y_train, \n",
    "                        'x_test': x_test, \n",
    "                        'y_test':y_test\n",
    "                        }\n",
    "    \n",
    "      #BGL\n",
    "    print(\"load BGL\")\n",
    "    (x_train, y_train), (x_test, y_test) = BGL.loadDataset('Drain_result/BGL.log_structured.csv', window='sliding', time_interval=3600*6, stepping_size=3600, train_ratio=0.7)\n",
    "    datasets['BGL'] = {'x_train':x_train, 'y_train': y_train, 'x_test': x_test, 'y_test':y_test}\n",
    "   \n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a270bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = loadDatasets()\n",
    "for d in datasets:\n",
    "    datasets[d]['contamination'] = sum(datasets[d]['y_train'])/len(datasets[d]['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abeb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalAndAddToBenchmark(modelName, model, dataName, data_x, data_y, data_unseen = False, traintime=0):\n",
    "\n",
    "    print('Train accuracy:')\n",
    "    start = timeit.default_timer()\n",
    "    precision, recall, f1 = model.evaluate(data_x, data_y)\n",
    "    stop = timeit.default_timer()\n",
    "    evaltime = stop - start\n",
    "    \n",
    "    total = len(data_y)\n",
    "    anomaly = sum(data_y)\n",
    "    normal = total - anomaly\n",
    "    \n",
    "    \n",
    "    benchmark_results.append([modelName, dataName, data_unseen, total, normal, anomaly, precision, recall, f1, evaltime, traintime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "\n",
    "for d in datasets:\n",
    "    train = datasets[d]['y_train']\n",
    "    test  = datasets[d]['y_test']\n",
    "    ds.append([d,'train', len(train), len(train)-sum(train), sum(train)])\n",
    "    ds.append([d,'test', len(test), len(test)-sum(test), sum(test)])\n",
    "\n",
    "dsf = pd.DataFrame(ds, columns=['dataset','type','total', 'normal', 'anomaly'])\n",
    "dsf.to_csv(\"result_data/data-comperison.csv\")\n",
    "g = sns.barplot(\n",
    "    data=dsf,\n",
    "    x=\"dataset\", \n",
    "    y=\"total\", \n",
    "    hue=\"type\",\n",
    "    ci=None, \n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "g = sns.barplot(\n",
    "    data=dsf,\n",
    "    x=\"dataset\", \n",
    "    y=\"anomaly\", \n",
    "    hue=\"type\",\n",
    "    palette=\"dark\",\n",
    "    ci=None, \n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "\n",
    "g.set_title(\"Datensätze im Vergleich\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc8334-e19f-4ce7-ba57-f6f927f85f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(map(len, datasets['HDFS']['x_train']))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca0df3-d18d-471c-8097-51dba07878a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef8ee4-8a40-443a-93d3-dcc61457f912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f8556-deb9-4fbf-ae27-2f67ca0c066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loglizer.utils import metrics\n",
    "\n",
    "class UnionModel():\n",
    "    def __init__(self,model_list):\n",
    "        self.models = model_list\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        for m in self.models:\n",
    "            m.fit(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for m in self.models:\n",
    "            y_pred = [ a or b for (a,b) in zip(y_pred,m.predict(X)) ] \n",
    "        return list(y_pred)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1\n",
    "\n",
    "class IntersectionModel():\n",
    "    def __init__(self,model_list):\n",
    "        self.models = model_list\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        for m in self.models:\n",
    "            m.fit(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.ones(len(X))\n",
    "        for m in self.models:\n",
    "            #y_pred = map(operator.add, m.predict(X))\n",
    "            y_pred = [ a and b for (a,b) in zip(y_pred,m.predict(X)) ] \n",
    "        return list(y_pred)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0131ddf-d0dd-4d94-9fb5-e4620e418f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = []\n",
    "\n",
    "feature_extractor = preprocessing.FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f635f0-2ec9-43c2-a5f3-65c55b0057ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['PCA', 'LOF', 'iForest', 'KNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94872618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    \n",
    "    print(\"Ussing dataset: \" + d )\n",
    "    \n",
    "    x_train = datasets[d]['x_train']\n",
    "    y_train = datasets[d]['y_train']\n",
    "    x_test = datasets[d]['x_test']\n",
    "    y_test = datasets[d]['y_test']\n",
    "    \n",
    "    contamination = datasets[d]['contamination']\n",
    "\n",
    "    x_train_extracted = feature_extractor.fit_transform(x_train, term_weighting='tf-idf', normalization='zero-mean')\n",
    "    x_test_extracted = feature_extractor.transform(x_test)\n",
    "    for m in models:\n",
    "        \n",
    "        print(\"Evaluationg Algorithmn: \" + m)\n",
    "        start = timeit.default_timer()\n",
    "    \n",
    "        if m == 'KNN':\n",
    "            model = KNN(n_neighbors=39, contamination=contamination)\n",
    "            model.fit(x_train_extracted)\n",
    "        elif m == 'PCA':\n",
    "            model = PCA(c_alpha=2.5)\n",
    "            model.fit(x_train_extracted)\n",
    "        elif m == 'InvariantMiner':\n",
    "            model = InvariantsMiner(epsilon=0.5)\n",
    "            model.fit(x_train_extracted)\n",
    "        elif m == 'LOF':\n",
    "            model = LOF(n_neighbors=60, leaf_size=60, contamination=contamination)\n",
    "            model.fit(x_train_extracted)\n",
    "            model.novelty = True\n",
    "        elif m == 'iForest':\n",
    "            model = IsolationForest(random_state=2019, max_samples=0.9999, contamination=contamination)\n",
    "            model.fit(x_train_extracted)\n",
    "        elif m == 'DeepLog':\n",
    "            # TODO DeepLog in den Dataframe einbinden \n",
    "            batch_size = 32\n",
    "            num_workers = 1\n",
    "            \n",
    "            vectorizer = Vectorizer()\n",
    "            train_dataset = vectorizer.fit_transform(x_train, window_y_train, y_train)\n",
    "            test_dataset = vectorizer.transform(x_test, window_y_test, y_test)\n",
    "\n",
    "            train_loader = Iterator(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers).iter\n",
    "            test_loader = Iterator(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers).iter\n",
    "            \n",
    "            model = DeepLog(num_labels=vectorizer.num_labels)\n",
    "            model.fit(train_loader)\n",
    "            \n",
    "            stop = timeit.default_timer()\n",
    "            traintime = stop - start\n",
    "            \n",
    "            print('Train accuracy:')\n",
    "            start = timeit.default_timer()\n",
    "            metrics = model.evaluate(train_loader)\n",
    "            stop = timeit.default_timer()\n",
    "            evaltime = stop - start\n",
    "            \n",
    "            print('Train accuracy:')\n",
    "            start = timeit.default_timer()\n",
    "            metrics2 = model.evaluate(test_loader)\n",
    "            stop = timeit.default_timer()\n",
    "            evaltime = stop - start\n",
    "            continue\n",
    "        elif m == 'Union':\n",
    "            pca = PCA()\n",
    "            iforest = IsolationForest(random_state=2022, max_samples=0.9999, contamination=contamination)\n",
    "            lof = LOF(n_neighbors=60, leaf_size=60, contamination='auto')\n",
    "            model = UnionModel([pca,iforest,lof])\n",
    "            model.fit(x_train_extracted)\n",
    "        elif m == 'Intersection':\n",
    "            pca = PCA()\n",
    "            iforest = IsolationForest(random_state=2019, max_samples=0.9999, contamination=contamination)\n",
    "            model = IntersectionModel([pca,iforest])\n",
    "            model.fit(x_train_extracted)\n",
    "        else: continue \n",
    "            \n",
    "        stop = timeit.default_timer()\n",
    "        traintime = stop - start\n",
    "        evalAndAddToBenchmark(m, model, d, x_train_extracted, y_train, data_unseen=False, traintime=traintime)\n",
    "        evalAndAddToBenchmark(m, model, d, x_test_extracted, y_test, data_unseen=True,traintime=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"algorithm\",\"dataset\", \"unseen_data\",\"data_total\",\"data_normal\",\"data_anomaly\",\"accuracy\",\"recall\",\"f1\", \"evaltime\", \"traintime\"]\n",
    "df = pd.DataFrame(benchmark_results,columns=columns)\n",
    "df.to_csv('result_data/model_comperison_last.csv')\n",
    "t = str(datetime.datetime.utcnow())\n",
    "df.to_csv('result_data/model_comperison_' + t +'.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b793c7-2459-427d-9581-1163278f828e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ea044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df[\"unseen_data\"] == True] \n",
    "df_train = df[df[\"unseen_data\"] == False] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_test, kind=\"bar\",\n",
    "    x=\"algorithm\", y=\"accuracy\", hue=\"dataset\", hue_order=['HDFS','BGL','Thunderbird'],\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Präzision\")\n",
    "g.legend.set_title(\"\")\n",
    "g.savefig('result_data/precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_test, kind=\"bar\",\n",
    "    x=\"algorithm\", y=\"recall\", hue=\"dataset\", hue_order=['HDFS','BGL','Thunderbird'],\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Recall\")\n",
    "g.legend.set_title(\"\")\n",
    "g.savefig('result_data/recall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_test, kind=\"bar\",\n",
    "    x=\"algorithm\", y=\"f1\", hue=\"dataset\", hue_order=['HDFS','BGL','Thunderbird'],\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"F1-score\")\n",
    "g.legend.set_title(\"\")\n",
    "g.savefig('result_data/f1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_train, kind=\"bar\",\n",
    "    x=\"algorithm\", y=\"traintime\", hue=\"dataset\", hue_order=['HDFS','BGL','Thunderbird','ABC'],\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Laufzeit\")\n",
    "g.legend.set_title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c97a5d-49bb-400b-bbe2-5cf12270f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_latex('result_data/model-comperison.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9ecf9-dc1d-46f0-bc34-21657a6d43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df[df['unseen_data'] == False].get(['dataset', 'algorithm', 'accuracy', 'traintime'])\n",
    "g.to_latex('result_data/runtime.tex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
