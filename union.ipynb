{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e3f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "import datetime\n",
    "\n",
    "from loglizer.loglizer.models.DeepLog import DeepLog\n",
    "from loglizer.loglizer.models.PCA import PCA\n",
    "from loglizer.loglizer.models.LOF import LocalOutlierFactor as LOF\n",
    "from loglizer.loglizer.models.KNeighbors import KNeighbors as KNN\n",
    "from loglizer.loglizer.models.LogClustering import LogClustering\n",
    "from loglizer.loglizer.models.InvariantsMiner import InvariantsMiner\n",
    "from loglizer.loglizer.models.IsolationForest import IsolationForest\n",
    "from loglizer.loglizer.models import DeepLog\n",
    "from loglizer.loglizer.preprocessing import Vectorizer, Iterator\n",
    "from loglizer.loglizer import preprocessing\n",
    "from loglizer.loglizer.dataloader import HDFS, BGL, Thunderbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDatasets():\n",
    "    datasets = {}\n",
    "  \n",
    "    #Thunderbird\n",
    "    print(\"load Thunderbird\")\n",
    "    (x_train, y_train), (x_test, y_test) = Thunderbird.loadDataset('Drain_result/Thunderbird_10m.log_structured.csv', window='sliding', time_interval=3600*0.5, stepping_size=60*5, train_ratio=0.7)\n",
    "    datasets['Thunderbird'] = {'x_train':x_train, 'y_train': y_train, 'x_test': x_test, 'y_test':y_test}\n",
    "    \n",
    "    \n",
    "    #HDFS\n",
    "    print(\"load HDFS\")\n",
    "    struct_log = 'Drain_result/HDFS.log_structured.csv'\n",
    "    labels = 'logs/HDFS/anomaly_label.csv'\n",
    "    (x_train, y_train), (x_test, y_test) = HDFS.loadDataset(struct_log,\n",
    "                                                           label_file=labels,\n",
    "                                                           window='session',\n",
    "                                                           train_ratio=0.7,\n",
    "                                                           split_type='uniform')\n",
    "    \n",
    "    datasets['HDFS'] = {'x_train':x_train, \n",
    "                        'y_train': y_train, \n",
    "                        'x_test': x_test, \n",
    "                        'y_test':y_test\n",
    "                        }\n",
    "    \n",
    "    #BGL\n",
    "    print(\"load BGL\")\n",
    "    (x_train, y_train), (x_test, y_test) = BGL.loadDataset('Drain_result/BGL.log_structured.csv', window='sliding', time_interval=3600*6, stepping_size=3600, train_ratio=0.7)\n",
    "    datasets['BGL'] = {'x_train':x_train, 'y_train': y_train, 'x_test': x_test, 'y_test':y_test}\n",
    "   \n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a270bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = loadDatasets()\n",
    "for d in datasets:\n",
    "    datasets[d]['contamination'] = sum(datasets[d]['y_train'])/len(datasets[d]['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abeb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalAndAddToBenchmark(modelName, model, dataName, data_x, data_y, data_unseen = False, traintime=0):\n",
    "\n",
    "    print('Train accuracy:')\n",
    "    start = timeit.default_timer()\n",
    "    precision, recall, f1 = model.evaluate(data_x, data_y)\n",
    "    stop = timeit.default_timer()\n",
    "    evaltime = stop - start\n",
    "    \n",
    "    total = len(data_y)\n",
    "    anomaly = sum(data_y)\n",
    "    normal = total - anomaly\n",
    "    \n",
    "    \n",
    "    benchmark_results.append([modelName, dataName, data_unseen, total, normal, anomaly, precision, recall, f1, evaltime, traintime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "\n",
    "for d in datasets:\n",
    "    train = datasets[d]['y_train']\n",
    "    test  = datasets[d]['y_test']\n",
    "    ds.append([d,'train', len(train), len(train)-sum(train), sum(train)])\n",
    "    ds.append([d,'test', len(test), len(test)-sum(test), sum(test)])\n",
    "\n",
    "dsf = pd.DataFrame(ds, columns=['dataset','type','total', 'normal', 'anomaly'])\n",
    "dsf.to_csv(\"result_data/model-comperison.csv\")\n",
    "g = sns.barplot(\n",
    "    data=dsf,\n",
    "    x=\"dataset\", \n",
    "    y=\"total\", \n",
    "    hue=\"type\",\n",
    "    ci=None, \n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "g = sns.barplot(\n",
    "    data=dsf,\n",
    "    x=\"dataset\", \n",
    "    y=\"anomaly\", \n",
    "    hue=\"type\",\n",
    "    palette=\"dark\",\n",
    "    ci=None, \n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "\n",
    "g.set_title(\"Datensätze im Vergleich\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f8556-deb9-4fbf-ae27-2f67ca0c066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loglizer.utils import metrics\n",
    "\n",
    "class UnionModel():\n",
    "    def __init__(self,model_list):\n",
    "        self.models = model_list\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        for m in self.models:\n",
    "            m.fit(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for m in self.models:\n",
    "            #y_pred = map(operator.add, m.predict(X))\n",
    "            y_pred = [ a or b for (a,b) in zip(y_pred,m.predict(X)) ] \n",
    "        return list(y_pred)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1\n",
    "\n",
    "class IntersectionModel():\n",
    "    def __init__(self,model_list):\n",
    "        self.models = model_list\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        for m in self.models:\n",
    "            m.fit(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.ones(len(X))\n",
    "        for m in self.models:\n",
    "            #y_pred = map(operator.add, m.predict(X))\n",
    "            y_pred = [ a and b for (a,b) in zip(y_pred,m.predict(X)) ] \n",
    "        return list(y_pred)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        print('====== Evaluation summary ======')\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f1 = metrics(y_pred, y_true)\n",
    "        print('Precision: {:.3f}, recall: {:.3f}, F1-measure: {:.3f}\\n'.format(precision, recall, f1))\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f026c8-b641-4a7d-8458-8185473e585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(2,len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0131ddf-d0dd-4d94-9fb5-e4620e418f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = []\n",
    "\n",
    "feature_extractor = preprocessing.FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94872618",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['PCA', 'LOF', 'iForest', 'KNN']\n",
    "for d in datasets:\n",
    "    \n",
    "    print(\"Ussing dataset: \" + d )\n",
    "    \n",
    "    x_train = datasets[d]['x_train']\n",
    "    y_train = datasets[d]['y_train']\n",
    "    x_test = datasets[d]['x_test']\n",
    "    y_test = datasets[d]['y_test']\n",
    "    \n",
    "    contamination = datasets[d]['contamination']\n",
    "\n",
    "    x_train_extracted = feature_extractor.fit_transform(x_train, term_weighting='tf-idf', normalization='zero-mean')\n",
    "    x_test_extracted = feature_extractor.transform(x_test)\n",
    "    \n",
    "    pca = PCA(c_alpha=2.5)\n",
    "    pca.fit(x_train_extracted)\n",
    "    iforest = IsolationForest(random_state=2019, max_samples=0.9999, contamination=contamination)\n",
    "    iforest.fit(x_train_extracted)\n",
    "    lof = LOF(n_neighbors=60, leaf_size=60, contamination=contamination)\n",
    "    lof.fit(x_train_extracted)\n",
    "    knn = KNN(n_neighbors=39, contamination=contamination)\n",
    "    knn.fit(x_train_extracted)\n",
    "\n",
    "    \n",
    "    for m in powerset(models):\n",
    "        l = []\n",
    "        if 'PCA' in m: l.append(pca)\n",
    "        if 'LOF' in m: l.append(lof)\n",
    "        if 'KNN' in m: l.append(knn)\n",
    "        if 'iForest' in m: l.append(iforest)\n",
    "        \n",
    "        print(\"Evaluationg Algorithmn: \" + str(m))\n",
    "        start = timeit.default_timer()\n",
    "        model = UnionModel(l)\n",
    "        #model.fit(x_train_extracted)\n",
    "            \n",
    "        stop = timeit.default_timer()\n",
    "        traintime = stop - start\n",
    "        evalAndAddToBenchmark(m, model, d, x_train_extracted, y_train, data_unseen=False, traintime=traintime)\n",
    "        evalAndAddToBenchmark(m, model, d, x_test_extracted, y_test, data_unseen=True,traintime=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8fb2f-83cf-4df3-867c-9de8d4eb2f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"algorithm\",\"dataset\", \"unseen_data\",\"data_total\",\"data_normal\",\"data_anomaly\",\"accuracy\",\"recall\",\"f1\", \"evaltime\", \"traintime\"]\n",
    "df = pd.DataFrame(benchmark_results,columns=columns)\n",
    "df.to_csv('result_data/union_model-comperison2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ea044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df[\"unseen_data\"] == True] \n",
    "df_train = df[df[\"unseen_data\"] == False] \n",
    "\n",
    "def pretty(t):\n",
    "    t = t.replace(\"'\", \"\")\n",
    "    t = t.replace('(', \"\")\n",
    "    t = t.replace(')', \"\")\n",
    "    t = t.replace(',',  \" $\\cup$\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_test, kind=\"bar\",\n",
    "    y=\"algorithm\", x=\"accuracy\", hue=\"dataset\",\n",
    "    hue_order=['HDFS','BGL','Thunderbird', 'ABC'],\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "lables = list(map(lambda t: pretty(t.get_text()) ,g.ax.get_yticklabels()))\n",
    "g.set_yticklabels(lables)\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Präzision\")\n",
    "g.legend.set_title(\"\")\n",
    "g.savefig('result_data/union_precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_test, kind=\"bar\",\n",
    "    y=\"algorithm\", x=\"recall\", hue=\"dataset\",\n",
    "    hue_order=['HDFS','BGL','Thunderbird', 'ABC'],\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "\n",
    "lables = list(map(lambda t: pretty(t.get_text()) ,g.ax.get_yticklabels()))\n",
    "g.set_yticklabels(lables)\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Recall\")\n",
    "g.legend.set_title(\"\")\n",
    "g.savefig('result_data/union_recall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_test, kind=\"bar\",\n",
    "    y=\"algorithm\", x=\"f1\", hue=\"dataset\",\n",
    "    hue_order=['HDFS','BGL','Thunderbird','ABC'],\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "\n",
    "lables = list(map(lambda t: pretty(t.get_text()) ,g.ax.get_yticklabels()))\n",
    "g.set_yticklabels(lables)\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"F1-score\")\n",
    "g.legend.set_title(\"\")\n",
    "g.savefig('result_data/union_f1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df_train, kind=\"bar\",\n",
    "    y=\"algorithm\", x=\"traintime\", hue=\"dataset\",\n",
    "    ci=None, alpha=1, height=6\n",
    ")\n",
    "\n",
    "lables = list(map(lambda t: pretty(t.get_text()) ,g.ax.get_yticklabels()))\n",
    "g.set_yticklabels(lables)\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Laufzeit\")\n",
    "g.legend.set_title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c97a5d-49bb-400b-bbe2-5cf12270f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.get(['algorithm','dataset','unseen_data', 'accuracy', 'recall', 'f1', 'evaltime', 'traintime']).to_latex('result_data/union.tex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
